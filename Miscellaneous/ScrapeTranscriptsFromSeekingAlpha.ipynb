{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib, sys\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from collections import Counter\n",
    "import time\n",
    "import csv\n",
    "import pandas as pd\n",
    "from urllib.error import HTTPError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Call Transcripts from: STI\n",
      "http://seekingalpha.com//article/4212825-suntrust-banks-inc-sti-ceo-bill-rogers-q3-2018-results-earnings-call-transcript?part=single\n",
      "Extracting Call Transcripts from: USB\n",
      "http://seekingalpha.com//article/4212234-u-s-bancorp-usb-ceo-andrew-cecere-q3-2018-results-earnings-call-transcript?part=single\n",
      "Extracting Call Transcripts from: RF\n",
      "http://seekingalpha.com//article/4213610-regions-financial-corporation-rf-ceo-john-turner-q3-2018-results-earnings-call-transcript?part=single\n",
      "Extracting Call Transcripts from: FITB\n",
      "http://seekingalpha.com//article/4213496-fifth-third-bancorp-fitb-ceo-greg-carmichael-q3-2018-results-earnings-call-transcript?part=single\n"
     ]
    }
   ],
   "source": [
    "\n",
    "symbol_list=['STI','USB','RF','FITB']\n",
    "# df=pd.DataFrame(columns=['ticker','word','count'])\n",
    "transcriptText = ''\n",
    "\n",
    "for symbol in symbol_list:\n",
    "    site='http://seekingalpha.com/symbol/'+symbol+'/earnings/transcripts'\n",
    "#     print(symbol)\n",
    "\n",
    "    hdr = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11',\n",
    "       'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "       'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
    "       'Accept-Encoding': 'none',\n",
    "       'Accept-Language': 'en-US,en;q=0.8',\n",
    "       'Connection': 'keep-alive'}\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        pageReq=urllib.request.Request(site,headers=hdr) \n",
    "        page=urllib.request.urlopen(pageReq).read()\n",
    "\n",
    "#         page=urllib.urlopen(req)\n",
    "        soup=BeautifulSoup(page,'lxml')\n",
    "        for link in soup.find_all('a'):\n",
    "            x=link.get('href')\n",
    "#             print(isinstance(x,str))\n",
    "            if isinstance(x,str):\n",
    "            # Parameterization Pending\n",
    "                wordlist=['transcript','2018','q3']\n",
    "                \n",
    "                if all(x.find(s)>=0 for s in wordlist):\n",
    "                    parse_site='http://seekingalpha.com/'+x\n",
    "                    parse_site='http://seekingalpha.com/'+x+'?part=single'\n",
    "                    print('Extracting Call Transcripts from: ' + symbol)\n",
    "                    print(parse_site)\n",
    "#                     parse_req=urllib.request.urlopen(parse_site)\n",
    "\n",
    "                    # Open the URL as Browser, not as python urllib\n",
    "                    pageAct=urllib.request.Request(url,headers=hdr) \n",
    "                    infileAct=urllib.request.urlopen(pageAct).read()\n",
    "            \n",
    "                    soup = BeautifulSoup(infileAct, 'html5lib') \n",
    "                    \n",
    "                    try:\n",
    "                        # Get content from article body only\n",
    "                        text = soup.findAll('div', attrs = {'id':'a-body'})\n",
    "                        # print(text)\n",
    "\n",
    "                        for p in text:\n",
    "                            # Extract all the text from within the p tags\n",
    "#                             print(p.text)\n",
    "                            transcriptText += p.text\n",
    "                        \n",
    "                    except HTTPError as e:\n",
    "                            print('Error Code', e.code)\n",
    "                    transcriptText.replace('\\n', ' ')\n",
    "                    \n",
    "                    fileName = \"Call_Transcripts_Seeking_Alpha/CallTranscripts\" + symbol + \".txt\"\n",
    "                    with open(fileName, \"w\") as text_file:\n",
    "                        print(transcriptText, file=text_file)\n",
    "                        \n",
    "\n",
    "    \n",
    "                    time.sleep(5)\n",
    "    except HTTPError as e:\n",
    "        print('Error code:',e.code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# url= 'http://seekingalpha.com//article/4212234-u-s-bancorp-usb-ceo-andrew-cecere-q3-2018-results-earnings-call-transcript?part=single'\n",
    "# # print(site)\n",
    "\n",
    "# # url = \"http://www.londonstockexchange.com/exchange/prices-and-markets/stocks/indices/ftse-indices.html\"\n",
    "\n",
    "# hdr = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11',\n",
    "#        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "#        'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
    "#        'Accept-Encoding': 'none',\n",
    "#        'Accept-Language': 'en-US,en;q=0.8',\n",
    "#        'Connection': 'keep-alive'}\n",
    "\n",
    "\n",
    "# # Open the URL as Browser, not as python urllib\n",
    "# page=urllib.request.Request(url,headers=hdr) \n",
    "# infile=urllib.request.urlopen(page).read()\n",
    "# data = infile.decode('ISO-8859-1') # Read the content as string decoded with ISO-8859-1\n",
    "\n",
    "# print(data) # Print the data to the screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transcriptText.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# soup = BeautifulSoup(infile, 'html5lib') \n",
    "# print(soup.prettify()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = soup.findAll('div', attrs = {'id':'a-body'})\n",
    "# # print(text)\n",
    "\n",
    "# for x in text:\n",
    "#     print(x.text.strip('\\n'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
