{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib, sys\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from collections import Counter\n",
    "import time\n",
    "import csv\n",
    "import pandas as pd\n",
    "from urllib.error import HTTPError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Call Transcripts from: STT\n",
      "http://seekingalpha.com//article/4212850-state-street-corporation-stt-ceo-jay-hooley-q3-2018-results-earnings-call-transcript?part=single\n",
      "               PublishedTime Symbol  \\\n",
      "0  Oct. 19, 2018  4:15 PM ET    STT   \n",
      "\n",
      "                                             summary  \\\n",
      "0  State Street Corporation (NYSE:STT) Q3 2018 Re...   \n",
      "\n",
      "                                               title  \n",
      "0  State Street Corporation (STT) CEO Jay Hooley ...  \n",
      "Extracting Call Transcripts from: GE\n",
      "http://seekingalpha.com//article/4215813-general-electric-ge-q3-2018-results-earnings-call-transcript?part=single\n",
      "               PublishedTime Symbol  \\\n",
      "0  Oct. 30, 2018 11:20 AM ET     GE   \n",
      "\n",
      "                                             summary  \\\n",
      "0  State Street Corporation (NYSE:STT) Q3 2018 Re...   \n",
      "\n",
      "                                               title  \n",
      "0  General Electric (GE) Q3 2018 Results - Earnin...  \n"
     ]
    }
   ],
   "source": [
    "#''GE','PEP', 'MHCP',STT\n",
    "symbol_list=['STT', 'GE']\n",
    "# df=pd.DataFrame(columns=['ticker','word','count'])\n",
    "transcriptText = ''\n",
    "\n",
    "file_name = \"seek_alpha_ws_test.txt\"\n",
    "\n",
    "f=open(file_name, \"w\")\n",
    "\n",
    "for symbol in symbol_list:\n",
    "    site='http://seekingalpha.com/symbol/'+symbol+'/earnings/transcripts'\n",
    "#     print(symbol)\n",
    "\n",
    "    hdr = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11',\n",
    "       'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "       'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
    "       'Accept-Encoding': 'none',\n",
    "       'Accept-Language': 'en-US,en;q=0.8',\n",
    "       'Connection': 'keep-alive'}\n",
    "    \n",
    "    \n",
    "    \n",
    "    pageReq=urllib.request.Request(site,headers=hdr) \n",
    "    page=urllib.request.urlopen(pageReq).read()\n",
    "\n",
    "#         page=urllib.urlopen(req)\n",
    "    soup=BeautifulSoup(page,'lxml')\n",
    "    for link in soup.find_all('a'):\n",
    "        x=link.get('href')\n",
    "#             print(isinstance(x,str))\n",
    "        if isinstance(x,str):\n",
    "            # Parameterization Pending\n",
    "            wordlist=['transcript','2018','q3']\n",
    "                \n",
    "            if all(x.find(s)>=0 for s in wordlist):\n",
    "                parse_site='http://seekingalpha.com/'+x\n",
    "                parse_site='http://seekingalpha.com/'+x+'?part=single'\n",
    "                print('Extracting Call Transcripts from: ' + symbol)\n",
    "                print(parse_site)\n",
    "#                     parse_req=urllib.request.urlopen(parse_site)\n",
    "\n",
    "                    # Open the URL as Browser, not as python urllib\n",
    "\n",
    "                pageAct=urllib.request.Request(parse_site,headers=hdr) \n",
    "                infileAct=urllib.request.urlopen(pageAct).read()\n",
    "            \n",
    "                soup = BeautifulSoup(infileAct, 'html5lib') \n",
    "                    #print(soup.prettify()) \n",
    "                    \n",
    "                    #print(\"yes\")\n",
    "                try:\n",
    "                        \n",
    "                    text = soup.find_all('div', attrs = {'id':'a-body'})\n",
    "\n",
    "                    for p in text:\n",
    "                            # Extract all the text from within the p tags\n",
    "                            #print(p.text)\n",
    "                        transcriptText += p.text\n",
    "                        #print(transcriptText)\n",
    "                    \n",
    "                        \n",
    "                except HTTPError as e:\n",
    "                        print('Error Code', e.code)\n",
    "                transcriptText.replace('\\n', ' ')\n",
    "                     \n",
    "                for texts in text:\n",
    "                    # fix summary part\n",
    "                    summary = \"\".join([texts.text for texts in soup.find_all(\"p\")])\n",
    "                        #print(summary)\n",
    "\n",
    "                    publishedTime = \"\".join([time.text for time in soup.find_all(\"time\", itemprop=\"datePublished\")])\n",
    "                        #print(publishedTime)\n",
    "\n",
    "                    title = \"\".join([title.text for title in soup.find_all(\"title\")])\n",
    "                        #print(title)\n",
    "\n",
    "                    df = pd.DataFrame({'Symbol': [symbol], 'PublishedTime': [publishedTime], 'title': [title], 'summary': [transcriptText] })\n",
    "                    print(df.head()) \n",
    "                    df.to_csv('test.csv',header=True,mode='a')\n",
    "                    time.sleep(5)\n",
    "\n",
    "                    f.write(\"Symbol: \\n\"+symbol+\"\\nTitle: \\n\"+title+\"\\nDate of document: \\n\"+publishedTime+\"\\nContent: \\n\"+summary+\"\\n+-+-+-+-+-+-+-+-+-+\\n\")\n",
    "\n",
    "\n",
    "        \n",
    "                    \n",
    "                    \n",
    "#                     fileName = \"/Users/akshitachawdhary/Desktop/DA_Davidson/Capstone_NLP_DIWD/Call_Transcripts_Seeking_Alpha/\" + symbol + \".txt\"\n",
    "#                     with open(fileName, \"w\") as text_file:\n",
    "#                         print(transcriptText, file=text_file)\n",
    "                        \n",
    "    \n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
