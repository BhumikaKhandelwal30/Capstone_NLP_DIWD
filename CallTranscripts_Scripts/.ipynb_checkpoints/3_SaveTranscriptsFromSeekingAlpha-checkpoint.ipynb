{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib, sys\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from collections import Counter\n",
    "import time\n",
    "import csv\n",
    "import pandas as pd\n",
    "from urllib.error import HTTPError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickerList = pd.read_excel('../CallTranscript_DownloadTemplate.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['transcript', '2019', 'q1']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordlist = ['transcript']\n",
    "wordlist.extend([str(tickerList['Year'][0]), str(tickerList['Quarter'][0])])\n",
    "wordlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://seekingalpha.com/symbol/GOOG/earnings/transcripts\n",
      "Extracting Call Transcripts from: GOOG\n",
      "http://seekingalpha.com//article/4257884-alphabet-inc-s-goog-ceo-sundar-pichai-q1-2019-results-earnings-call-transcript?part=single\n"
     ]
    }
   ],
   "source": [
    "symbol_list = list(tickerList['Ticker'])\n",
    "# symbol_list=['GOOG','GOOGL','GE','ETN', 'PEP', 'KHC', 'FB', 'MCHP', 'SLAB', 'JPM', 'STT']\n",
    "# df=pd.DataFrame(columns=['ticker','word','count'])\n",
    "transcriptText = ''\n",
    "\n",
    "for symbol in symbol_list:\n",
    "    site='http://seekingalpha.com/symbol/'+symbol+'/earnings/transcripts'\n",
    "    print(site)\n",
    "\n",
    "    hdr = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11',\n",
    "       'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "       'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
    "       'Accept-Encoding': 'none',\n",
    "       'Accept-Language': 'en-US,en;q=0.8',\n",
    "       'Connection': 'keep-alive'}\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        pageReq=urllib.request.Request(site,headers=hdr) \n",
    "        page=urllib.request.urlopen(pageReq).read()\n",
    "\n",
    "#         page=urllib.urlopen(req)\n",
    "        soup=BeautifulSoup(page,'lxml')\n",
    "        for link in soup.find_all('a'):\n",
    "            x=link.get('href')\n",
    "#             print(isinstance(x,str))\n",
    "            if isinstance(x,str):\n",
    "            # Parameterization Pending\n",
    "#                 wordlist=['transcript','2018','q4']\n",
    "                \n",
    "                if all(x.find(s)>=0 for s in wordlist):\n",
    "                    parse_site='http://seekingalpha.com/'+str(x)\n",
    "                    parse_site='http://seekingalpha.com/'+str(x)+'?part=single'\n",
    "                    print('Extracting Call Transcripts from: ' + symbol)\n",
    "                    print(parse_site)\n",
    "#                     parse_req=urllib.request.urlopen(parse_site)\n",
    "\n",
    "                    # Open the URL as Browser, not as python urllib\n",
    "                    pageAct=urllib.request.Request(parse_site,headers=hdr) \n",
    "                    infileAct=urllib.request.urlopen(pageAct).read()\n",
    "            \n",
    "                    soup = BeautifulSoup(infileAct, 'html5lib') \n",
    "                    \n",
    "                    try:\n",
    "                        # Get content from article body only\n",
    "                        text = soup.findAll('div', attrs = {'id':'a-body'})\n",
    "                        # print(text)\n",
    "\n",
    "                        for p in text:\n",
    "                            # Extract all the text from within the p tags\n",
    "#                             print(p.text)\n",
    "                            transcriptText += p.text\n",
    "                        \n",
    "                    except HTTPError as e:\n",
    "                            print('Error Code', e.code)\n",
    "                    transcriptText.replace('\\n', ' ')\n",
    "                    \n",
    "                    fileName = \"../Call_Transcripts_Seeking_Alpha/CallTranscripts\" + symbol + \"_\"+ wordlist[1]+wordlist[2]+ \".txt\"\n",
    "                    with open(fileName, \"w\") as text_file:\n",
    "                        print(transcriptText, file=text_file)\n",
    "                        \n",
    "\n",
    "    \n",
    "                    time.sleep(250)\n",
    "    except HTTPError as e:\n",
    "        print('Error code:',e.code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
